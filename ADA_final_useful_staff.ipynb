{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from files\n",
    "\n",
    "df_1 = pd.read_csv(\"./data/data.txt\",\n",
    "                 sep=\"\\n\",\n",
    "                 names=['column1'],\n",
    "                 dtype='string')\n",
    "\n",
    "df_2 = pd.read_csv(\"./data/data.csv\",\n",
    "                 sep=\",\",\n",
    "                 names=['column1', 'column2'],\n",
    "                 dtype={'column1': np.float64, 'column2':np.float64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe\n",
    "\n",
    "df = pd.DataFrame(columns=['A','B','C','D','E','F','G'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows\n",
    "\n",
    "# drop rows which are NAN\n",
    "df = df[df['A'].notna()]\n",
    "\n",
    "# drop rows which are in list ~ is used as a negate symbol\n",
    "df = df[~df['A'].isin([1,2,3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby()\n",
    "\n",
    "# will group values based on similar values for A and B\n",
    "df_gb_C_value = df.groupby(['A', 'B'])['C']\n",
    "\n",
    "df_gb_all_values = df.groupby(['A', 'B'])\n",
    " for enumerator, (key, item) in enumerate(df_gb_all_values):\n",
    "        index_of_c = item['C'].index\n",
    "        value_of_c = item['C'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting term frequency in a column\n",
    "\n",
    "df['A'].str.split(expand=True).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90378bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots with axis names\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['A'],bins=50)\n",
    "plt.title(\"TITLE\",fontsize=30)\n",
    "plt.ylabel('Y LABELS',fontsize=25)\n",
    "plt.xlabel('X LABEL',fontsize=25)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['A'],bins=50,  log=\"y\")\n",
    "plt.title(\"TITLE\",fontsize=30)\n",
    "plt.ylabel('Y LABELS',fontsize=25)\n",
    "plt.xlabel('X LABEL',fontsize=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append \n",
    "\n",
    "ans = []\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    ans.append(i*2)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ba45d",
   "metadata": {},
   "source": [
    "### 01 - Handling data/Happiness.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38865b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All names to lower case\n",
    "\n",
    "happy_data.country.str.lower()\n",
    "\n",
    "# or\n",
    "\n",
    "def Name(s):\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "happy_data.country = happy_data.country.apply(Name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dfs \n",
    "\n",
    "country_features = happy_data.merge(c_data, left_on = 'country', right_on = 'country_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10\n",
    "\n",
    "country_features.sort_values(by = ['happiness_score'], ascending=False).head(10).country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format \n",
    "\n",
    "best = country_features[country_features.literacy == '100,0']\n",
    "\n",
    "def Form(s):\n",
    "    return(s.world_region + '-' + s.country + '(' + str(s.happiness_score) + ')')\n",
    "\n",
    "best['form'] = best.apply(Form, 1)\n",
    "best['form']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace . and , to get float\n",
    "\n",
    "country_features.literacy = country_features.literacy.str.replace(',', '.').astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b9ef6d",
   "metadata": {},
   "source": [
    "### 02 - Data viz and data from the web/Becoming a DataVizard exercise.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper bootstrapping function you can use in this exercise to obtain 95% confidence\n",
    "# intervals around the estimated average. The underlying concept will be explained in the next week's lecture.\n",
    "\n",
    "# Input: your array and the number of random samples (e.g., 1000 is a good number)\n",
    "# Output: [lower error, upper error]\n",
    "\n",
    "def bootstrap_CI(data, nbr_draws):\n",
    "    means = np.zeros(nbr_draws)\n",
    "    data = np.array(data)\n",
    "\n",
    "    for n in range(nbr_draws):\n",
    "        indices = np.random.randint(0, len(data), len(data))\n",
    "        data_tmp = data[indices] \n",
    "        means[n] = np.nanmean(data_tmp)\n",
    "\n",
    "    return [np.nanpercentile(means, 2.5),np.nanpercentile(means, 97.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b838fd",
   "metadata": {},
   "source": [
    "#### Just look there for vizualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c194b4",
   "metadata": {},
   "source": [
    "### 03 - Describing data/Describing_data_exercise.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random rows from data frame\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistic\n",
    "\n",
    "df['IncomePerCap'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24532208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for normal distribution\n",
    "\n",
    "diagnostic.kstest_normal(df['IncomePerCap'].values, dist = 'norm')\n",
    "\n",
    "# p_value < 0.05 -> we can reject the null hypothesis that the data comes from a normal distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for exponential distribution\n",
    "diagnostic.kstest_normal(df['IncomePerCap'].values, dist = 'exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data \n",
    "\n",
    "#make 10 samples with replacement\n",
    "sample1_counties = df.sample(n = 10, replace = True)\n",
    "\n",
    "#make 10 samples without replacement\n",
    "sample1_counties = df.sample(n = 10, replace = False)\n",
    "\n",
    "#sometimes we want to sample in an ublanaced way, so that we upsample datapoints of certain characteristic,\n",
    "#and downsample the others. this can be acieved with weights parameter\n",
    "#here we sample by upsampling counties with large population\n",
    "sample2_counties = df.sample(n = 10, replace = False, weights = df['TotalPop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ff1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "\n",
    "stats.pearsonr(df['IncomePerCap'],df['Employed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .loc\n",
    "\n",
    "df.loc[df['State'] == 'New York']['IncomePerCap'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-sided t-test\n",
    "\n",
    "stats.ttest_ind(df.loc[df['State'] == 'New York']['IncomePerCap'], \n",
    "                df.loc[df['State'] == 'California']['IncomePerCap'])\n",
    "\n",
    "# This is a two-sided test for the null hypothesis that the two independent samples have identical average (expected) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR\n",
    "\n",
    "df_A = df[(df.State == 'Wisconsin') | (df.State == 'Tennessee') | (df.State == 'Minnesota')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f86c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter with line and CI\n",
    "\n",
    "sns.lmplot(x='SelfEmployed', y = 'IncomePerCap', data = df_A, hue = 'State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cb645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  In a new groundbreaking study, 28 out of 100 patients reported improvements after taking a new medicine. \n",
    "# It is known that, when given a placebo, 20% of the patients report feeling better. \n",
    "# What is the p-value assuming the null hypothesis that the probability of successful treatment in this \n",
    "# groundbreaking study is the same as the probability of reporting feeling better under placebo, \n",
    "# according to a one-sided binomial test? Hint: you may use the statsmodels.stats.proportion.binom_test function.\n",
    "\n",
    "import statsmodels.stats\n",
    "import statsmodels.stats.proportion\n",
    "statsmodels.stats.proportion.binom_test(28, 100, prop=0.2, alternative='larger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b400e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping function.\n",
    "# It should take an array and the number of iterations as inputs, and output 95% confidence intervals of the mean.\n",
    "\n",
    "def Boots_are_made_for_walking(x, N, k):\n",
    "    means=[]\n",
    "    for i in range(N):\n",
    "        means.append(x.sample(k, replace = True).mean())\n",
    "    means = np.array(means)\n",
    "    mu = means.mean()\n",
    "    s = means.std()\n",
    "    CI_lower = mu - 1.96*s\n",
    "    CI_higher = mu + 1.96*s\n",
    "    CI = [CI_lower, CI_higher]\n",
    "    means.sort()\n",
    "    CI = [np.percentile(means, 2.5), np.percentile(means, 97.5)]\n",
    "    return mu, CI\n",
    "\n",
    "Boots_are_made_for_walking(df['MeanCommute'], 10000, k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentiles\n",
    "\n",
    "df['MeanCommute'].describe(percentiles=[.025, 0.5, 0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd08ba9",
   "metadata": {},
   "source": [
    "### 04 - Regression analysis/Regression_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declares the model\n",
    "\n",
    "mod = smf.ols(formula='time ~ C(diabetes) + C(high_blood_pressure)', data=df)\n",
    "\n",
    "# Fits the model (find the optimal coefficients, adding a random seed ensures consistency)\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "\n",
    "# Print thes summary output provided by the library.\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction terms: a*b to add terms: a, b, a:b, and intercept\n",
    "\n",
    "mod = smf.ols(formula='time ~ high_blood_pressure * DEATH_EVENT + diabetes',\n",
    "              data=df)\n",
    "\n",
    "\n",
    "res = mod.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c080436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the countinuous variables\n",
    "\n",
    "df['age'] = (df['age'] - df['age'].mean())/df['age'].std()\n",
    "\n",
    "# Standardize multiple columns\n",
    "\n",
    "num_cols = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium']\n",
    "df.loc[:,num_cols] = (df[num_cols] - df[num_cols].mean())/df[num_cols].std()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit is logistic regression\n",
    "\n",
    "mod = smf.logit(formula='''DEATH_EVENT ~  age + creatinine_phosphokinase + ejection_fraction + \n",
    "                        platelets + serum_creatinine + serum_sodium + \n",
    "                        C(diabetes) + C(high_blood_pressure) +\n",
    "                        C(sex) + C(anaemia) + C(smoking) + C(high_blood_pressure)''', data=df)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names\n",
    "variables = res.params.index\n",
    "\n",
    "# quantifying uncertainty!\n",
    "\n",
    "# coefficients\n",
    "coefficients = res.params.values\n",
    "\n",
    "# p-values\n",
    "p_values = res.pvalues\n",
    "\n",
    "# standard errors\n",
    "standard_errors = res.bse.values\n",
    "\n",
    "#confidence intervals\n",
    "res.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of all the predictors\n",
    "\n",
    "# sort them all by coefficients\n",
    "l1, l2, l3, l4 = zip(*sorted(zip(coefficients[1:], variables[1:], standard_errors[1:], p_values[1:])))\n",
    "\n",
    "# in this case, we index starting from the first element, not to plot the intercept\n",
    "\n",
    "# we will use standard errors, instead of CIs\n",
    "# two standard errors approximate the CIs (you can actually see in the summary table that\n",
    "# +/2 SI is equivalent to the CIs)\n",
    "\n",
    "#fancy plotting\n",
    "\n",
    "plt.errorbar(l1, np.array(range(len(l1))), xerr= 2*np.array(l3), linewidth = 1,\n",
    "             linestyle = 'none',marker = 'o',markersize= 3,\n",
    "             markerfacecolor = 'black',markeredgecolor = 'black', capsize= 5)\n",
    "\n",
    "plt.vlines(0,0, len(l1), linestyle = '--')\n",
    "\n",
    "plt.yticks(range(len(l2)),l2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8935048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize log odds\n",
    "\n",
    "np.seterr(divide = 'ignore') \n",
    "fig, axs = plt.subplots(1, 3, figsize=(14,3))\n",
    "p = np.linspace(0, 0.99, 1000)\n",
    "odds = p/(1-p)\n",
    "axs[0].set_title(\"p vs. odds\")\n",
    "axs[0].plot(p, odds)\n",
    "axs[0].set_xlabel(\"p\")\n",
    "axs[0].set_ylabel(\"odds\")\n",
    "\n",
    "axs[1].set_title(\"odds vs. log-odds\")\n",
    "axs[1].plot(odds, np.log(odds))\n",
    "axs[1].set_xlabel(\"odds\")\n",
    "axs[1].set_ylabel(\"log odds\")\n",
    "\n",
    "axs[2].set_title(\"p vs. log-odds\")\n",
    "axs[2].plot(p, np.log(odds))\n",
    "axs[2].set_xlabel(\"p\")\n",
    "axs[2].set_ylabel(\"log odds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81730d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log odds\n",
    "\n",
    "log(1/(1-p1)) + 0.66 = log(1/(1-p2))\n",
    "log(1) - log(1-p1) + 0.66 = log(1) - log(1-p2)\n",
    "log(1-p1) - log(1-p2) = 0.66\n",
    "(1-p1)/(1-p2) = e^0.66\n",
    "If p1 = 0.1, find p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation\n",
    "\n",
    "mod2 = smf.ols('np.log(time) ~ high_blood_pressure + diabetes + DEATH_EVENT', data = df)\n",
    "res2 = mod2.fit()\n",
    "res2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906829c",
   "metadata": {},
   "source": [
    "### 05 - Observational studies/Observational studies exercise.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a53ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy histogram\n",
    "\n",
    "sns.histplot(x = 're78', data = df, hue = 'treat', multiple = 'dodge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy Violine plot\n",
    "\n",
    "sns.violinplot(x = 'treat', y = 're78', data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy scatter with hue\n",
    "\n",
    "plt.figure(figsize=(5, 3), dpi=100)\n",
    "sns.scatterplot(x='age', y='educ', data=df, hue='treat', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction\n",
    "\n",
    "probs = res.predict(exog = df)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the dataset via matching\n",
    "\n",
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)\n",
    "\n",
    "# Separate the treatment and control groups\n",
    "treatment_df = df[df['treat'] == 1]\n",
    "control_df = df[df['treat'] == 0]\n",
    "\n",
    "# Create an empty undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Loop through all the pairs of instances\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        # Calculate the similarity \n",
    "        similarity = get_similarity(control_row['PS'],\n",
    "                                    treatment_row['PS'])\n",
    "\n",
    "        # Add an edge between the two instances weighted by the similarity between them\n",
    "        G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "# Generate and return the maximum weight matching on the generated graph\n",
    "matching = nx.max_weight_matching(G)\n",
    "\n",
    "matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "matched[0:5] #it's just the index of every element we matched. Others we've trashed\n",
    "# Use iloc to use indices\n",
    "\n",
    "df_b = df.iloc[matched]\n",
    "df_b.groupby(['treat']).mean() #balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the groups further\n",
    "\n",
    "treatment_df = lalonde_data[lalonde_data['treat'] == 1]\n",
    "control_df = lalonde_data[lalonde_data['treat'] == 0]\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        # Adds an edge only if the individuals have the same race\n",
    "        if (control_row['black'] == treatment_row['black'])\\\n",
    "            and (control_row['hispan'] == treatment_row['hispan']):\n",
    "            similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                        treatment_row['Propensity_score'])\n",
    "\n",
    "            G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "matching_race = nx.max_weight_matching(G)\n",
    "matched_race = [i[0] for i in list(matching_race)] + [i[1] for i in list(matching_race)]\n",
    "df_r = df.iloc[matched_race]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to write formulas nicely\n",
    "\n",
    "f = '''re78 ~ treat + age + educ + black + \n",
    "                            hispan + married + nodegree + re74 + re75'''\n",
    "\n",
    "smf.ols(formula = f, data = df).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9611a46",
   "metadata": {},
   "source": [
    "### 06 - Supervised Learning/SupervisedLearning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6913fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between the features and the response\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, sharey=True)\n",
    "data.plot(kind='scatter', x='TV', y='sales', ax=axs[0], figsize=(16, 5), grid=True)\n",
    "data.plot(kind='scatter', x='radio', y='sales', ax=axs[1], grid=True)\n",
    "data.plot(kind='scatter', x='newspaper', y='sales', ax=axs[2], grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae80255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn provides an easy way to train the model:\n",
    "\n",
    "lin_reg = LinearRegression()  # create the model\n",
    "lin_reg.fit(X, y)  # train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fde12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions and the original values:\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([min(y), max(y)], [min(y), max(y)], 'r--', lw=4)\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb17e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation is above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "\n",
    "mean_squared_error(y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78cd35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization with Ridge\n",
    "\n",
    "ridge = Ridge(alpha=6)\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "predicted_r = cross_val_predict(ridge, X, y, cv=5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y, predicted_r, edgecolors=(0, 0, 0))\n",
    "ax.plot([min(y), max(y)], [min(y), max(y)], 'r--', lw=4)\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ddd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummies\n",
    "\n",
    "titanic_features = ['sex', 'age', 'sibsp', 'parch', 'fare']\n",
    "X = pd.get_dummies(titanic[titanic_features])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdac6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the missing values with the mean.\n",
    "\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "len(X[X.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99186ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and recall\n",
    "\n",
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")\n",
    "\n",
    "# Precision: avoid false positives\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (precision.mean(), precision.std() * 2))\n",
    "# Recall: avoid false negatives\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (recall.mean(), recall.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "\n",
    "# Predict the probabilities with a cross validationn\n",
    "y_pred = cross_val_predict(logistic, X, y, cv=10, method=\"predict_proba\")\n",
    "# Compute the False Positive Rate and True Positive Rate\n",
    "fpr, tpr, _ = roc_curve(y, y_pred[:, 1])\n",
    "# Compute the area under the fpt-tpf curve\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Area = {:.5f}\".format(auc_score));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd844529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain prediction for specific points\n",
    "\n",
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "logistic.fit(X, y)\n",
    "\n",
    "# dictionary\n",
    "# ex = {'age':25, 'sibsp': 0, 'parch': 0, 'fare': 100, 'sex_female': 0, 'sex_male': 1}\n",
    "# logistic.predict(exog=ex)\n",
    "# this is from statsmodels\n",
    "\n",
    "# In sklearn:\n",
    "ex = np.array([[25, 0, 0, 100, 0, 1]])\n",
    "logistic.predict_proba(ex)\n",
    "\n",
    "# What about a woman, 35 years old, alone onboard and with the same fare?\n",
    "\n",
    "ex = np.array([[35, 0, 0, 100, 1, 0]])\n",
    "logistic.predict_proba(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217aa96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes in logit\n",
    "\n",
    "logistic.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "from sklearn.datasets import make_moons, make_gaussian_quantiles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_moons, y_moons = make_moons(500, noise=0.2, random_state=0)\n",
    "X_circles, y_circles = make_gaussian_quantiles(n_samples=100, random_state=0)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(9,4))\n",
    "\n",
    "axs[0].scatter(X_moons[:,0], X_moons[:,1], c=y_moons)\n",
    "axs[0].set_title('Moon Data')\n",
    "\n",
    "axs[1].scatter(X_circles[:,0], X_circles[:,1], c=y_circles)\n",
    "axs[1].set_title('Circles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34f062",
   "metadata": {},
   "source": [
    "#### Look for more KNN in the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest \n",
    "\n",
    "# Load the data\n",
    "titanic = pd.read_excel('data/titanic.xls')\n",
    "titanic_features = ['sex', 'age', 'sibsp', 'parch', 'fare']\n",
    "X = pd.get_dummies(titanic[titanic_features])\n",
    "X = X.fillna(X.mean())\n",
    "y = titanic['survived']\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "number_trees = [n for n in range(1, 21)]\n",
    "precision_scores = []\n",
    "recalls_scores = []\n",
    "\n",
    "\n",
    "for nt in number_trees:\n",
    "    clf = RandomForestClassifier(max_depth=3, random_state=0, n_estimators=nt)\n",
    "    clf.fit(X, y)\n",
    "    precision = cross_val_score(clf, X, y, cv=10, scoring=\"precision\")\n",
    "    precision_scores.append(precision.mean())\n",
    "    recall = cross_val_score(clf, X, y, cv=10, scoring=\"recall\")\n",
    "    recalls_scores.append(recall.mean())\n",
    "    \n",
    "fig, ax = plt.subplots(1, figsize=(6,4))\n",
    "\n",
    "ax.plot(number_trees, precision_scores, label=\"Precision\")\n",
    "ax.plot(number_trees, recalls_scores, label=\"Recall\")\n",
    "\n",
    "ax.set_ylabel(\"Score value\")\n",
    "ax.set_xlabel(\"Number of trees\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857ada1",
   "metadata": {},
   "source": [
    "### 07 - Applied ML/AppliedML.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36847444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies\n",
    "\n",
    "df = original_data.copy()\n",
    "\n",
    "categorical_columns = ['sex_upon_outcome', 'animal_type', 'intake_condition',\n",
    "                       'intake_type', 'sex_upon_intake']\n",
    "df = pd.get_dummies(original_data, columns=categorical_columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f575314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set lables\n",
    "\n",
    "def Lable(s):\n",
    "    if s == 'Adoption':\n",
    "        s = 1\n",
    "    else: s = 0\n",
    "    return s\n",
    "\n",
    "df.outcome_type = df.outcome_type.apply(Lable) \n",
    "df.outcome_type.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20528828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, train_size=0.8, random_state=42)\n",
    "\n",
    "len(df_train)/(len(df_train) + len(df_test))\n",
    "\n",
    "# If external libraries are not allowed:\n",
    "idx = df.sample(round(0.8*df.shape[0])).index\n",
    "df_train = df.loc[idx]\n",
    "idx_not = [i for i in df.index if i not in idx]\n",
    "df_test = df.loc[idx_not]\n",
    "df_test\n",
    "\n",
    "# From Solutions:\n",
    "\n",
    "train_label = df_train.outcome_type\n",
    "train_features = df_train.drop('outcome_type', axis=1)\n",
    "\n",
    "\n",
    "test_label = df_test.outcome_type\n",
    "test_features = df_test.drop(columns=['outcome_type'])\n",
    "\n",
    "means = train_features.mean()\n",
    "stddevs = train_features.std()\n",
    "\n",
    "train_features_standardized = pd.DataFrame()\n",
    "\n",
    "for c in train_features.columns:\n",
    "    train_features_standardized[c] = (train_features[c]-means[c])/stddevs[c]\n",
    "\n",
    "# Use the mean and stddev of the training set\n",
    "test_features_standardized = pd.DataFrame()\n",
    "\n",
    "for c in test_features.columns:\n",
    "    test_features_standardized[c] = (test_features[c]-means[c])/stddevs[c]\n",
    "\n",
    "train_features_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression classifier with threshold\n",
    "\n",
    "logistic = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "logistic.fit(train_features_standardized, train_label)\n",
    "\n",
    "prediction_proba = logistic.predict_proba(test_features_standardized)\n",
    "\n",
    "prediction_proba[0]\n",
    "\n",
    "\n",
    "logistic.classes_\n",
    "# So we have that the prob of the first element to be 0 is 0.68, and the prob of it being 1 is 0.32.\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "def PredictResult(probas, threshold):\n",
    "    ans = []\n",
    "    for i in probas:\n",
    "        if i[1] > threshold:\n",
    "            ans.append(1)\n",
    "        else:\n",
    "            ans.append(0)\n",
    "    return ans\n",
    "\n",
    "prediction = PredictResult(prediction_proba, threshold = 0.5)\n",
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19840a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(test_label, prediction)\n",
    "\n",
    "# In the binary case, we can extract true positives, etc as follows:\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_label, prediction).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05837b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, precision, recall, F1\n",
    "\n",
    "accuracy =  (tp+tn)/(tn + fp + fn + tp)\n",
    "\n",
    "precision_positive = tp/(tp+fp) if (tp+fp) !=0 else np.nan\n",
    "precision_negative = tn/(tn+fn) if (tn+fn) !=0 else np.nan\n",
    "\n",
    "recall_positive = tp/(tp+fn) if (tp+fn) !=0 else np.nan\n",
    "recall_negative = tn/(tn+fp) if (tn+fp) !=0 else np.nan\n",
    "\n",
    "F1_score_positive = 2 *(precision_positive*recall_positive)/(precision_positive+recall_positive) if (precision_positive+recall_positive) !=0 else np.nan\n",
    "F1_score_negative = 2 *(precision_negative*recall_negative)/(precision_negative+recall_negative) if (precision_negative+recall_negative) !=0 else np.nan\n",
    "\n",
    "print(\"The accuracy of this model is {0:1.3f}\".format(accuracy))\n",
    "print(\"For the positive case, the precision is {0:1.3f}, the recall is {1:1.3f} and the F1 score is {2:1.3f}\"\\\n",
    "      .format(precision_positive, recall_positive, F1_score_positive))\n",
    "print(\"For the negative case, the precision is {0:1.3f}, the recall is {1:1.3f} and the F1 score is {2:1.3f}\"\\\n",
    "      .format(precision_negative, recall_negative, F1_score_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de89cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary the value of the threshold in the range from 0 to 1 and visualize the value of accuracy, precision, recall, and F1-score (with respect to both classes) as a function of the threshold.\n",
    "\n",
    "threshold = np.linspace(0, 1, 100)\n",
    "\n",
    "accuracy_list = []\n",
    "precision_positive_list = []\n",
    "precision_negative_list = []\n",
    "recall_positive_list = []\n",
    "recall_negative_list = []\n",
    "F1_score_positive_list = []\n",
    "F1_score_negative_list = []\n",
    "\n",
    "for t in threshold:\n",
    "    prediction = PredictResult(prediction_proba, t)\n",
    "    tn, fp, fn, tp = confusion_matrix(test_label, prediction).ravel()\n",
    "    accuracy =  (tp+tn)/(tn + fp + fn + tp)\n",
    "    precision_positive = tp/(tp+fp) if (tp+fp) !=0 else np.nan\n",
    "    precision_negative = tn/(tn+fn) if (tn+fn) !=0 else np.nan\n",
    "    recall_positive = tp/(tp+fn) if (tp+fn) !=0 else np.nan\n",
    "    recall_negative = tn/(tn+fp) if (tn+fp) !=0 else np.nan\n",
    "    F1_score_positive = 2 *(precision_positive*recall_positive)/(precision_positive+recall_positive) if (precision_positive+recall_positive) !=0 else np.nan\n",
    "    F1_score_negative = 2 *(precision_negative*recall_negative)/(precision_negative+recall_negative) if (precision_negative+recall_negative) !=0 else np.nan\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_positive_list.append(precision_positive)\n",
    "    precision_negative_list.append(precision_negative)\n",
    "    recall_positive_list.append(recall_positive)\n",
    "    recall_negative_list.append(recall_negative)\n",
    "    F1_score_positive_list.append(F1_score_positive)\n",
    "    F1_score_negative_list.append(F1_score_negative)\n",
    "    \n",
    "plt.plot(threshold, accuracy_list)\n",
    "plt.grid()\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b466b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(10,5))\n",
    "\n",
    "axs[0, 0].plot(threshold, precision_positive_list)\n",
    "axs[0, 0].set_title('Precision_positive')\n",
    "\n",
    "axs[0, 1].plot(threshold, recall_positive_list)\n",
    "axs[0, 1].set_title('Recall_positive')\n",
    "\n",
    "axs[0, 2].plot(threshold, F1_score_positive_list)\n",
    "axs[0, 2].set_title('F1_score_positive')\n",
    "\n",
    "axs[1, 0].plot(threshold, precision_negative_list)\n",
    "axs[1, 0].set_title('Precision_negative')\n",
    "\n",
    "axs[1, 1].plot(threshold, recall_negative_list)\n",
    "axs[1, 1].set_title('Recall_negative')\n",
    "\n",
    "axs[1, 2].plot(threshold, F1_score_negative_list)\n",
    "axs[1, 2].set_title('F1_score_negative')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='threshold')\n",
    "    ax.grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98314231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot in a bar chart the coefficients of the logistic regression sorted by their contribution to the prediction.\n",
    "\n",
    "logistic = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "logistic.fit(train_features_std, train_label)\n",
    "\n",
    "tmp = []\n",
    "for name, value in zip(train_features_std.columns, logistic.coef_[0]):\n",
    "    tmp.append({\"name\": name, \"value\": value})\n",
    "    \n",
    "features_coef = pd.DataFrame(tmp).sort_values(\"value\")\n",
    "features_coef\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(5,7))\n",
    "plt.barh(features_coef.name, features_coef.value, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa477e",
   "metadata": {},
   "source": [
    "### 08 - Unsupervised learning/Unsupervised_Learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb44c4",
   "metadata": {},
   "source": [
    "#### Just look at the tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c551d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of several features\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(10,5), sharey=True)\n",
    "\n",
    "for i in range(7):\n",
    "    if i < 4:\n",
    "        axs[0,i].hist(df.columns[i], data = df, bins = 20, alpha = 0.7)\n",
    "        axs[0,i].set_title(df.columns[i])\n",
    "    else:\n",
    "        axs[1,i-4].hist(df.columns[i], data = df, bins = 20, alpha = 0.7)\n",
    "        axs[1,i-4].set_title(df.columns[i])\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize with a function\n",
    "\n",
    "df = StandardScaler().fit(df).transform(df)\n",
    "pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd00d9",
   "metadata": {},
   "source": [
    "### ada-2021-homework-1-sublinear-lime-main/hw1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d20014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read compressed file\n",
    "\n",
    "df = pd.read_csv('data/data.tsv.gz', compression='gzip', sep='\\t', on_bad_lines='skip')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f34c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "df = df.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axis of the summation\n",
    "\n",
    "df['baseline_average'] = df[['baseline_motorcycle1', 'baseline_motorcycle2', \n",
    "                             'baseline_motorcycle3', 'baseline_motorcycle4']].sum(axis=1)/df['motorcycle_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc8a7d",
   "metadata": {},
   "source": [
    "#### Useful for fancy distribution plots and groupping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe36b9",
   "metadata": {},
   "source": [
    "### ada-2021-homework-2-sublinear-lime-main/hw2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8f7a2",
   "metadata": {},
   "source": [
    "#### Useful for Gradient Boosting, Residuals plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances\n",
    "\n",
    "# did_style_change\n",
    "\n",
    "# standardized euclidean distance of music-related numerical \n",
    "# features between the second and the first album\n",
    "\n",
    "edist = []\n",
    "music = [\"key\", \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\", \n",
    "         \"loudness\", \"speechiness\", \"valence\", \"tempo\"]\n",
    "\n",
    "df_music = df_12[music]\n",
    "df_music_1 = df_music[0::2]\n",
    "df_music_2 = df_music[1::2]\n",
    "v = df_music.var() #calculate variances, for euclidean\n",
    "\n",
    "n = df_music_1.shape[0]\n",
    "\n",
    "# seuclidean is for vectors, not matrices, so iterate\n",
    "edist = []\n",
    "for i in range(n):\n",
    "    dd = seuclidean(df_music_1.iloc[i], df_music_2.iloc[i], v)\n",
    "    edist.append(dd)\n",
    "    \n",
    "#create new df with data\n",
    "df_new = pd.DataFrame({'score_diff':score_diff, 'time_diff':time_diff, 'edist': edist})\n",
    "\n",
    "# assign 1 to the 20% most distant 1st-2nd album pairs and 0 to all others.\n",
    "df_new = df_new.sort_values('edist', ascending=False, ignore_index=True)\n",
    "\n",
    "df_new['did_style_change'] = 0\n",
    "\n",
    "border = int(n*0.2)-1 #correct, loc[0:n] gives n+1 elements\n",
    "df_new.loc[0:border, 'did_style_change'] = 1\n",
    "    \n",
    "df_new = df_new.drop(columns = 'edist')\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ded7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice cycle\n",
    "\n",
    "# Sort by artist and then by year\n",
    "df = df.sort_values(['artist', 'releaseyear'])\n",
    "\n",
    "# For each artist write that the earliest album is the \"first\" and one after that is the \"second\". \n",
    "\n",
    "df['first_or_sec'] = \"\"      \n",
    "        \n",
    "a = 0\n",
    "while a<df.shape[0]:\n",
    "    df.first_or_sec.iloc[a] = 'first'\n",
    "    df.first_or_sec.iloc[a+1] = 'second'\n",
    "    a += df.album_number.iloc[a]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Sort by artist and then by year\n",
    "# Take only those with 3 or more albums\n",
    "df_1234 = df[df.album_number >= 3].sort_values(['artist', 'releaseyear'])\n",
    "\n",
    "\n",
    "# Same logic as in task 5\n",
    "df_1234['first_to_fourth'] = \"\"\n",
    "\n",
    "a = 0\n",
    "while a<df_1234.shape[0]:\n",
    "    df_1234.first_to_fourth.iloc[a] = 'first'\n",
    "    df_1234.first_to_fourth.iloc[a+1] = 'second'\n",
    "    df_1234.first_to_fourth.iloc[a+2] = 'third'\n",
    "    df_1234.first_to_fourth.iloc[a+3] = 'fourth'\n",
    "    a += df_1234.album_number.iloc[a]\n",
    "\n",
    "\n",
    "df_1234 = df_1234[['artist', 'album_number', 'first_to_fourth', 'score']]\n",
    "df_1234 = df_1234[df_1234.first_to_fourth != \"\"]\n",
    "df_1234.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ad2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "\n",
    "band = 1000\n",
    "talent = np.random.uniform(2, 8, band)\n",
    "\n",
    "album_quality = []\n",
    "for i in range(band):\n",
    "    s = np.random.normal(talent[i], 1)\n",
    "    album_quality.append(s)\n",
    "    \n",
    "df_sim = pd.DataFrame({'band':range(band), 'talent':talent, 'album_quality':album_quality})\n",
    "\n",
    "df_sim['album'] = \"\"\n",
    "\n",
    "for b in range(len(df_sim.band)):\n",
    "    df_sim.album.iloc[b] = 'first'\n",
    "\n",
    "df_sim\n",
    "\n",
    "# For each of the remaining bands, create two additional albums\n",
    "\n",
    "df_sim2 = df_sim[['band', 'talent']]\n",
    "\n",
    "df_sim2['album_quality'] = 0\n",
    "for i in range(df_sim2.shape[0]):\n",
    "    s = np.random.normal(df_sim2.talent.iloc[i], 1)\n",
    "    df_sim2.album_quality.iloc[i] = s\n",
    "    \n",
    "df_sim2['album'] = \"\"\n",
    "for i in range(df_sim2.shape[0]):\n",
    "    df_sim2.album.iloc[i] = 'second'\n",
    "    \n",
    "df_sim3 = df_sim[['band', 'talent']]\n",
    "df_sim3['album_quality'] = 0 \n",
    "for i in range(df_sim3.shape[0]):\n",
    "    s = np.random.normal(df_sim3.talent.iloc[i], 1)\n",
    "    df_sim3.album_quality.iloc[i] = s\n",
    "    \n",
    "df_sim3['album'] = \"\"\n",
    "for i in range(df_sim3.shape[0]):\n",
    "    df_sim3.album.iloc[i] = 'third'\n",
    "\n",
    "df_sim = df_sim.append([df_sim2, df_sim3], ignore_index=True)\n",
    "df_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd307de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, plot the distributions of the three albums.\n",
    "\n",
    "plt.figure(dpi=100, figsize=(9, 6))\n",
    "ax = sns.histplot(x='album_quality', data=df_sim,\n",
    "            hue='album', multiple='dodge', shrink=0.8, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116956b",
   "metadata": {},
   "source": [
    "### Exams\n",
    "\n",
    "There is a huge class imbalance, thus, accuracy is not a good metric. It is very easy for accuracy to be biased. If you label everything as the majority class, the accuracy would still be very high (in the high 90s), but the performance on the minority class would be terrible.\n",
    "\n",
    "Average degree is not recommended as the degree distribution of real-world networks usually follows a powerlaw. Summarizing powerlaws with average values is not a good idea, as there is a long tail, and there are many nodes that have very high degree. Instead, median is a better choice.\n",
    "\n",
    "In-degree is the number of connections that point inward at a vertex. Out-degree is the number of connections that originate at a vertex and point outward to other vertices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e74b37",
   "metadata": {},
   "source": [
    "### Exam 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date and time format\n",
    "\n",
    "youtube['upload_year'] = youtube.upload_date.apply(lambda x: x.year)\n",
    "youtube['upload_month'] = youtube.upload_date.apply(lambda x: x.month)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
